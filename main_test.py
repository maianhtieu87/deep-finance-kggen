import os
import pandas as pd
from configs.config import GlobalConfig
from data_pipeline.fetchers.yahoo_fetcher import YahooFetcher
from data_pipeline.processors.price_processor import PriceProcessor
from data_pipeline.processors.macro_processor import MacroProcessor
from data_pipeline.processors.news_processor import NewsProcessor, NewsEmbedder
from data_pipeline.builder import DatasetBuilder

def run_test_pipeline_skipping_news_fetch():
    print("üöÄ STARTING TEST PIPELINE (Skipping News Fetching)...")

    # --- SETUP PATHS ---
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file k·∫øt qu·∫£ c≈© b·∫°n ƒë√£ c√≥
    # L∆ØU √ù: ƒê·∫£m b·∫£o file n√†y n·∫±m ƒë√∫ng v·ªã tr√≠ ho·∫∑c b·∫°n s·ª≠a ƒë∆∞·ªùng d·∫´n t·∫°i ƒë√¢y
    EXISTING_NEWS_PATH = os.path.join(GlobalConfig.INTERIM_PATH, "concatenated_news_filtered.parquet")
    
    if not os.path.exists(EXISTING_NEWS_PATH):
        print(f"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y file t·∫°i {EXISTING_NEWS_PATH}")
        print("   Vui l√≤ng copy file 'concatenated_news_filtered.parquet' v√†o th∆∞ m·ª•c 'data/interim/' ho·∫∑c c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n.")
        return

    # ==============================================================================
    # 1. Fetching Phase (Ch·ªâ l·∫•y Price & Macro, B·ªé QUA News Fetcher)
    # ==============================================================================
    print("\n--- Phase A: Fetching (Price & Macro only) ---")
    yahoo = YahooFetcher()
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs(GlobalConfig.RAW_PRICE_PATH, exist_ok=True)
    os.makedirs(GlobalConfig.RAW_MACRO_PATH, exist_ok=True)
    os.makedirs(GlobalConfig.PROCESSED_PATH, exist_ok=True)

    # 1.1 L·∫•y d·ªØ li·ªáu gi√° (L√†m x∆∞∆°ng s·ªëng cho tr·ª•c th·ªùi gian)
    print(f"   Downloading Price Data ({GlobalConfig.START_DATE} to {GlobalConfig.END_DATE})...")
    raw_price_list = yahoo.download_data(GlobalConfig.START_DATE, GlobalConfig.END_DATE, GlobalConfig.TICKERS)
    
    # 1.2 L·∫•y d·ªØ li·ªáu Vƒ© m√¥
    print("   Downloading Macro Indicators...")
    raw_macro = yahoo.fetch_macro_indicators(GlobalConfig.START_DATE, GlobalConfig.END_DATE, GlobalConfig.MACRO_SYMBOLS)

    # ==============================================================================
    # 2. Processing Phase (Load News c√≥ s·∫µn -> Align -> Embed)
    # ==============================================================================
    print("\n--- Phase B: Processing ---")
    price_proc = PriceProcessor()
    macro_proc = MacroProcessor()
    news_proc = NewsProcessor()

    # 2.1 X·ª≠ l√Ω Price & Macro
    print("   Processing Price & Macro...")
    price_dict = price_proc.combine_to_nested_dict(raw_price_list, GlobalConfig.TICKERS)
    processed_price_macro = macro_proc.process_and_enrich(price_dict, raw_macro)
    
    # L·∫•y danh s√°ch ng√†y giao d·ªãch chu·∫©n (Trading Days Backbone)
    trading_dates = list(processed_price_macro.keys())
    print(f"   Detected {len(trading_dates)} trading days.")

    # 2.2 Load Existing News
    print(f"   üì• Loading existing news from: {EXISTING_NEWS_PATH}")
    try:
        processed_news = pd.read_parquet(EXISTING_NEWS_PATH)
        print(f"   Loaded {len(processed_news)} news records.")
        
        # [ANNOTATION 1] Ki·ªÉm tra Schema
        required_cols = ['date', 'equity', 'title'] # C√°c c·ªôt b·∫Øt bu·ªôc cho b∆∞·ªõc sau
        missing_cols = [c for c in required_cols if c not in processed_news.columns]
        if missing_cols:
            print(f"   ‚ö†Ô∏è WARNING: File parquet thi·∫øu c√°c c·ªôt: {missing_cols}")
            print("   Logic c≈© c√≥ th·ªÉ d√πng t√™n kh√°c (v√≠ d·ª•: 'headline' thay v√¨ 'title'). ƒêang th·ª≠ t·ª± ƒë·ªông s·ª≠a...")
            if 'headline' in processed_news.columns and 'title' not in processed_news.columns:
                processed_news = processed_news.rename(columns={'headline': 'title'})
                print("   ‚úÖ Renamed 'headline' -> 'title'.")
            
            # Ki·ªÉm tra l·∫°i c·ªôt date ph·∫£i l√† datetime
            if not pd.api.types.is_datetime64_any_dtype(processed_news['date']):
                 processed_news['date'] = pd.to_datetime(processed_news['date']).dt.date
    
    except Exception as e:
        print(f"‚ùå ERROR reading parquet file: {e}")
        return

    # 2.3 Align News to Trading Days
    # B∆∞·ªõc n√†y v·∫´n C·∫¶N THI·∫æT v√¨ Price Data m·ªõi t·∫£i v·ªÅ c√≥ th·ªÉ c√≥ ng√†y ngh·ªâ l·ªÖ kh√°c ho·∫∑c range kh√°c
    print("   Aligning news to current Trading Days...")
    aligned_news = news_proc.align_to_trading_days(processed_news, trading_dates)
    print(f"   News after alignment: {len(aligned_news)} records.")

    # ==============================================================================
    # 3. Embedding Phase (Ch·∫°y Embedding tr√™n d·ªØ li·ªáu ƒë√£ load)
    # ==============================================================================
    print("\n--- Phase B.1: Embedding News ---")
    
    # [ANNOTATION 2] Ki·ªÉm tra file embedding c≈©
    embedder = NewsEmbedder()
    embedding_output_file = os.path.join(GlobalConfig.NEWS_EMBEDDING_OUTPUT_PATH, "embedded_news.json")
    
    if os.path.exists(embedding_output_file):
        print(f"   ‚ö†Ô∏è File embedding ƒë√£ t·ªìn t·∫°i: {embedding_output_file}")
        user_input = input("   B·∫°n c√≥ mu·ªën ch·∫°y l·∫°i Embedding (t·ªën ti·ªÅn/th·ªùi gian) kh√¥ng? (y/n): ")
        if user_input.lower() == 'y':
            embedding_json_path = embedder.process_and_save(aligned_news)
        else:
            print("   Skipping Embedding calculation. Using existing file.")
            embedding_json_path = embedding_output_file
    else:
        # N·∫øu ch∆∞a c√≥ file th√¨ ch·∫°y m·ªõi
        embedding_json_path = embedder.process_and_save(aligned_news)

    # ==============================================================================
    # 4. Building Phase (T·∫°o file Union cu·ªëi c√πng)
    # ==============================================================================
    print("\n--- Phase C: Building Union File ---")
    builder = DatasetBuilder()
    
    # Gi·∫£ ƒë·ªãnh file Filings ƒë√£ c√≥ (ho·∫∑c b·ªè qua n·∫øu ch∆∞a c·∫ßn test filings)
    filing_path = os.path.join(GlobalConfig.RAW_FILINGS_PATH, "final_summary_filing_data.parquet")
    if not os.path.exists(filing_path):
        print(f"   ‚ö†Ô∏è Warning: Filing file not found at {filing_path}. Creating dataset without filings.")
        # T·∫°o dummy empty dataframe ƒë·ªÉ code kh√¥ng l·ªói
        pd.DataFrame(columns=['filedAt', 'ticker', 'formType', 'content_summary']).to_parquet("dummy_filings.parquet")
        filing_path = "dummy_filings.parquet"

    dataset = builder.create_synchronized_data(
        processed_price_macro, 
        aligned_news, 
        filing_path,
        embedding_path=embedding_json_path
    )
    
    builder.save(dataset, filename='unified_dataset_test.pkl')
    
    # X√≥a file dummy n·∫øu c√≥
    if os.path.exists("dummy_filings.parquet"): os.remove("dummy_filings.parquet")
    
    print("\n‚úÖ TEST PIPELINE COMPLETED SUCCESSFULLY!")

if __name__ == "__main__":
    run_test_pipeline_skipping_news_fetch()