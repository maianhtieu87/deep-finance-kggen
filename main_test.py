# main_test.py

import os
import json
import pandas as pd

from configs.config import GlobalConfig
from data_pipeline.fetchers.yahoo_fetcher import YahooFetcher
from data_pipeline.processors.price_processor import PriceProcessor
from data_pipeline.processors.macro_processor import MacroProcessor
from data_pipeline.processors.news_processor import NewsProcessor, KGGenNewsEmbedder
from data_pipeline.builder import DatasetBuilder


def _quick_check_kg_index(kg_index_path: str, n_samples: int = 5) -> bool:
    """
    Sanity check format + existence of kg_tensor_path
    Expect:
      embedded_kg.json: { "YYYY-MM-DD": [ {"date":..., "equity":..., "kg_tensor_path":...}, ... ], ... }
    """
    if not os.path.exists(kg_index_path):
        print(f"‚ùå KG index not found: {kg_index_path}")
        return False

    try:
        with open(kg_index_path, "r", encoding="utf-8") as f:
            obj = json.load(f)
    except Exception as e:
        print(f"‚ùå Cannot read KG index JSON: {e}")
        return False

    if not isinstance(obj, dict) or len(obj) == 0:
        print("‚ùå KG index JSON is empty or not a dict.")
        return False

    # pick some records
    checked = 0
    missing = 0

    for date_str, recs in obj.items():
        if not isinstance(recs, list):
            continue
        for rec in recs:
            if not isinstance(rec, dict):
                continue
            if "kg_tensor_path" not in rec:
                continue
            checked += 1
            p = rec["kg_tensor_path"]
            if not isinstance(p, str) or not os.path.exists(p):
                missing += 1
            if checked >= n_samples:
                break
        if checked >= n_samples:
            break

    if checked == 0:
        print("‚ö†Ô∏è KG index has no usable records with 'kg_tensor_path'.")
        return False

    if missing > 0:
        print(f"‚ö†Ô∏è KG index check: {missing}/{checked} sampled tensor paths are missing.")
        print("   ‚Üí C√≥ th·ªÉ b·∫°n ƒë√£ move folder data/interim/kg ho·∫∑c rebuild ch∆∞a xong.")
        # v·∫´n return True v√¨ c√≥ th·ªÉ sample tr√∫ng missing; nh∆∞ng b·∫°n n√™n c√¢n nh·∫Øc
        return True

    print(f"‚úÖ KG index sanity-check OK. Sampled {checked} tensor paths exist.")
    return True


def run_test_pipeline_skipping_news_fetch():
    print("üöÄ STARTING TEST PIPELINE (Skipping News Fetching)...")

    EXISTING_NEWS_PATH = os.path.join(GlobalConfig.INTERIM_PATH, "concatenated_news_filtered.parquet")
    if not os.path.exists(EXISTING_NEWS_PATH):
        print(f"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y file t·∫°i {EXISTING_NEWS_PATH}")
        return

    # ===== PHASE A: PRICE + MACRO =====
    print("\n--- Phase A: Fetching (Price & Macro only) ---")
    yahoo = YahooFetcher()
    os.makedirs(GlobalConfig.RAW_PRICE_PATH, exist_ok=True)
    os.makedirs(GlobalConfig.RAW_MACRO_PATH, exist_ok=True)
    os.makedirs(GlobalConfig.PROCESSED_PATH, exist_ok=True)

    print(f"   Downloading Price Data ({GlobalConfig.START_DATE} to {GlobalConfig.END_DATE})...")
    raw_price_list = yahoo.download_data(
        GlobalConfig.START_DATE,
        GlobalConfig.END_DATE,
        GlobalConfig.TICKERS
    )

    print("   Downloading Macro Indicators...")
    raw_macro = yahoo.fetch_macro_indicators(
        GlobalConfig.START_DATE,
        GlobalConfig.END_DATE,
        GlobalConfig.MACRO_SYMBOLS
    )

    # ===== PHASE B: PROCESS =====
    print("\n--- Phase B: Processing ---")
    price_proc = PriceProcessor()
    macro_proc = MacroProcessor()
    news_proc = NewsProcessor()

    print("   Processing Price & Macro...")
    price_dict = price_proc.combine_to_nested_dict(raw_price_list, GlobalConfig.TICKERS)
    processed_price_macro = macro_proc.process_and_enrich(price_dict, raw_macro)

    trading_dates = list(processed_price_macro.keys())
    print(f"   Detected {len(trading_dates)} trading days.")

    print(f"   üì• Loading existing news from: {EXISTING_NEWS_PATH}")
    processed_news = pd.read_parquet(EXISTING_NEWS_PATH)
    print(f"   Loaded {len(processed_news)} news records.")

    if "headline" in processed_news.columns and "title" not in processed_news.columns:
        processed_news = processed_news.rename(columns={"headline": "title"})
        print("   ‚úÖ Renamed 'headline' -> 'title'.")

    if not pd.api.types.is_datetime64_any_dtype(processed_news["date"]):
        processed_news["date"] = pd.to_datetime(processed_news["date"]).dt.date

    print("   Aligning news to current Trading Days...")
    aligned_news = news_proc.align_to_trading_days(processed_news, trading_dates)
    print(f"   News after alignment: {len(aligned_news)} records.")

    # ===== PHASE B.1: KG OFFLINE (REUSE) =====
    print("\n--- Phase B.1: KG (reuse existing outputs by default) ---")

    kg_index_path = os.path.join(GlobalConfig.INTERIM_PATH, "kg_embeddings", "embedded_kg.json")

    if os.path.exists(kg_index_path):
        print(f"   ‚úÖ Found KG index: {kg_index_path}")

        ok = _quick_check_kg_index(kg_index_path, n_samples=5)
        if not ok:
            print("   ‚ö†Ô∏è KG index format/path seems problematic.")
            ans = input("   ‚Üí B·∫°n c√≥ mu·ªën rebuild KG l·∫°i t·ª´ ƒë·∫ßu (t·ªën LLM)? (y/n): ").strip().lower()
            if ans == "y":
                print("   üß® Rebuilding KG (LLM extraction + graph build)...")
                embedder = KGGenNewsEmbedder(
                    interim_root=GlobalConfig.INTERIM_PATH,
                    top_triples_per_article=5,
                    top_triples_per_day=None,   # gi·ªØ h·∫øt per-day (no top-k/day)
                    # NOTE: voyage resolution n·∫øu b·∫°n ƒë√£ implement trong news_processor
                    # v√† ƒë√£ set VOYAGE_API_KEY env. N·∫øu ch∆∞a, h√£y ƒë·ªÉ module t·ª± handle.
                )
                kg_index_path = embedder.process_and_save(aligned_news)
            else:
                print("   ‚ùå Kh√¥ng rebuild nh∆∞ng KG index hi·ªán kh√¥ng ·ªïn. D·ª´ng ƒë·ªÉ tr√°nh builder l·ªói.")
                return
        else:
            # ‚úÖ default: reuse (NO rebuild graph-only, NO voyage, NO llm)
            ans = input("   ‚Üí Reuse KG ƒë√£ build s·∫µn (skip build KG)? (y/n): ").strip().lower()
            if ans == "y":
                print("   ‚úÖ Reusing existing KG index. (NO LLM / NO Voyage / NO graph rebuild)")
            else:
                print("   üß® You chose to rebuild KG (LLM extraction + graph build)...")
                embedder = KGGenNewsEmbedder(
                    interim_root=GlobalConfig.INTERIM_PATH,
                    top_triples_per_article=5,
                    top_triples_per_day=None,   # gi·ªØ h·∫øt per-day (no top-k/day)
                )
                kg_index_path = embedder.process_and_save(aligned_news)
    else:
        print("   ‚ùå No KG index found.")
        ans = input("   ‚Üí Build KG now (t·ªën LLM)? (y/n): ").strip().lower()
        if ans != "y":
            print("   ‚ùå Kh√¥ng c√≥ KG index ƒë·ªÉ d√πng. D·ª´ng.")
            return
        embedder = KGGenNewsEmbedder(
            interim_root=GlobalConfig.INTERIM_PATH,
            top_triples_per_article=5,
            top_triples_per_day=None,
        )
        kg_index_path = embedder.process_and_save(aligned_news)

    embedding_json_path = kg_index_path

    # ===== PHASE C: FINAL UNION =====
    print("\n--- Phase C: Building Union File ---")
    builder = DatasetBuilder()

    filing_path = os.path.join(GlobalConfig.RAW_FILINGS_PATH, "final_summary_filing_data.parquet")
    if not os.path.exists(filing_path):
        print(f"   ‚ö†Ô∏è Warning: Filing file not found at {filing_path}. Creating dataset without filings.")
        pd.DataFrame(columns=["filedAt", "ticker", "formType", "content_summary"]).to_parquet("dummy_filings.parquet")
        filing_path = "dummy_filings.parquet"

    dataset = builder.create_synchronized_data(
        processed_price_macro,
        aligned_news,
        filing_path,
        embedding_path=embedding_json_path
    )

    builder.save(dataset, filename="unified_dataset_test.pkl")

    if os.path.exists("dummy_filings.parquet"):
        os.remove("dummy_filings.parquet")

    print("\n‚úÖ TEST PIPELINE COMPLETED SUCCESSFULLY!")


if __name__ == "__main__":
    run_test_pipeline_skipping_news_fetch()
