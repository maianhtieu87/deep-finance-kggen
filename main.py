import torch
import random
import numpy as np
import os
from src.model import StockMovementModel
from src.data_loader import data_prepare
from configs.config import TrainConfig

# --- 1. SETUP ---
def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

device = torch.device("cuda" if TrainConfig.use_cuda and torch.cuda.is_available() else "cpu")
set_seed(TrainConfig.seed)

# --- 2. HELPER: MERGE ---
def merge_datasets(list_of_dicts, shuffle=False):
    if not list_of_dicts: return {}
    keys = list_of_dicts[0].keys()
    merged_data = {}
    for key in keys:
        tensors = [d[key] for d in list_of_dicts if d and key in d]
        if tensors:
            merged_data[key] = torch.cat(tensors, dim=0)
    
    if shuffle and "label" in merged_data:
        indices = torch.randperm(len(merged_data["label"]))
        for key in merged_data:
            merged_data[key] = merged_data[key][indices]
    return merged_data

# ================================================================
# CHI·∫æN L∆Ø·ª¢C 1: SQRT BALANCING (KHUY·∫æN NGH·ªä)
# ================================================================
def compute_class_weights_sqrt(labels_tensor):
    """
    S·ª≠ d·ª•ng cƒÉn b·∫≠c hai c·ªßa inverse frequency.
    C√¥ng th·ª©c: Weight_i = sqrt(Total / Count_i) / mean(sqrt(...))
    
    ∆ØU ƒêI·ªÇM: 
    - Ph·∫°t nh·∫π h∆°n inverse frequency th√¥ng th∆∞·ªùng
    - V·∫´n gi·ªØ ƒë∆∞·ª£c xu h∆∞·ªõng c√¢n b·∫±ng
    - Tr√°nh weights qu√° c·ª±c ƒëoan
    
    V√ç D·ª§:
    Class counts: [50, 100, 50] 
    -> Inverse: [4.0, 2.0, 4.0] (ph·∫°t qu√° n·∫∑ng!)
    -> SQRT:    [1.41, 1.0, 1.41] (v·ª´a ph·∫£i)
    """
    labels = labels_tensor.cpu().numpy()
    class_counts = np.bincount(labels)
    total_samples = len(labels)
    
    # T√≠nh sqrt c·ªßa inverse frequency
    sqrt_weights = np.sqrt(total_samples / (class_counts + 1e-6))
    
    # Normalize v·ªÅ mean = 1
    normalized_weights = sqrt_weights / sqrt_weights.mean()
    
    return torch.tensor(normalized_weights, dtype=torch.float32)

# ================================================================
# CHI·∫æN L∆Ø·ª¢C 2: EFFECTIVE NUMBER OF SAMPLES (ENS)
# ================================================================
def compute_class_weights_ens(labels_tensor, beta=0.99):
    """
    Class-Balanced Loss Based on Effective Number of Samples (CVPR 2019).
    Paper: https://arxiv.org/abs/1901.05555
    
    C√¥ng th·ª©c: E_n = (1 - Œ≤^n) / (1 - Œ≤)
              Weight_i = (1 - Œ≤) / E_n_i
    
    THAM S·ªê:
    - beta=0.0: Kh√¥ng c√¢n b·∫±ng (gi·ªëng vanilla CE)
    - beta=0.9: C√¢n b·∫±ng nh·∫π
    - beta=0.99: C√¢n b·∫±ng v·ª´a (m·∫∑c ƒë·ªãnh)
    - beta=0.999: C√¢n b·∫±ng m·∫°nh
    
    √ù NGHƒ®A: 
    - Khi s·ªë l∆∞·ª£ng m·∫´u tƒÉng, hi·ªáu qu·∫£ h·ªçc t·∫≠p gi·∫£m d·∫ßn
    - M·∫´u th·ª© 100 kh√¥ng c√≤n quan tr·ªçng b·∫±ng m·∫´u ƒë·∫ßu ti√™n
    """
    labels = labels_tensor.cpu().numpy()
    class_counts = np.bincount(labels)
    
    effective_num = 1.0 - np.power(beta, class_counts)
    weights = (1.0 - beta) / (effective_num + 1e-6)
    
    # Normalize v·ªÅ t·ªïng = s·ªë l∆∞·ª£ng classes
    weights = weights / weights.sum() * len(class_counts)
    
    return torch.tensor(weights, dtype=torch.float32)

# ================================================================
# CHI·∫æN L∆Ø·ª¢C 3: INVERSE FREQUENCY CLIPPED (An to√†n h∆°n)
# ================================================================
def compute_class_weights_clipped(labels_tensor, max_ratio=10.0):
    """
    Inverse Frequency nh∆∞ng gi·ªõi h·∫°n t·ª∑ l·ªá max/min.
    
    THAM S·ªê:
    - max_ratio: T·ª∑ l·ªá t·ªëi ƒëa gi·ªØa weight l·ªõn nh·∫•t v√† nh·ªè nh·∫•t
    
    V√ç D·ª§:
    Class counts: [10, 100, 10]
    -> Inverse th√¥ng th∆∞·ªùng: [10, 1, 10] (ratio = 10x)
    -> Clipped (max_ratio=5): [5, 1, 5] (ratio = 5x)
    """
    labels = labels_tensor.cpu().numpy()
    class_counts = np.bincount(labels)
    total_samples = len(labels)
    num_classes = len(class_counts)
    
    # Inverse frequency
    weights = total_samples / (num_classes * class_counts + 1e-6)
    
    # Clip ƒë·ªÉ tr√°nh qu√° c·ª±c ƒëoan
    min_weight = weights.min()
    weights = np.minimum(weights, min_weight * max_ratio)
    
    return torch.tensor(weights, dtype=torch.float32)

# --- 3. EVALUATE ---
def evaluate(model, data_dict, return_details=False):
    """
    ƒê√°nh gi√° model v·ªõi option tr·∫£ v·ªÅ chi ti·∫øt predictions.
    
    Args:
        return_details: N·∫øu True, tr·∫£ v·ªÅ (acc, mcc, preds, targets)
    """
    if not data_dict: 
        return (0.0, 0.0, None, None) if return_details else (0.0, 0.0)
    
    model.eval()
    with torch.no_grad():
        s_o = data_dict["s_o"].to(device)
        s_h = data_dict["s_h"].to(device)
        s_c = data_dict["s_c"].to(device)
        s_m = data_dict["s_m"].to(device)
        s_n = data_dict["s_n"].to(device)
        label = data_dict["label"].to(device)
        
        if return_details:
            # Th·ª≠ d√πng return_preds n·∫øu model h·ªó tr·ª£, kh√¥ng th√¨ fallback
            try:
                acc, mcc, preds = model(s_o, s_h, s_c, s_m, s_n, label, mode="test", return_preds=True)
            except TypeError:
                # Model c≈© kh√¥ng h·ªó tr·ª£ return_preds
                acc, mcc = model(s_o, s_h, s_c, s_m, s_n, label, mode="test")
                # T√≠nh preds th·ªß c√¥ng
                with torch.no_grad():
                    v_m, v_i, v_n = model.multimodal_encoder(s_o, s_h, s_c, s_m, s_n)
                    fused_news = model.fusion_news(primary=v_i, aux=v_n)
                    fused_macro = model.fusion_macro(primary=v_i, aux=v_m)
                    v_fused_total = (fused_news + fused_macro) / 2.0
                    logits = model.movement_predictor(fused_seq=v_fused_total, orig_seq=v_i)
                    preds = torch.argmax(logits, dim=1)
            
            return acc, mcc, preds, label
        else:
            acc, mcc = model(s_o, s_h, s_c, s_m, s_n, label, mode="test")
            return acc, mcc

# --- 4. DETAILED PREDICTION ANALYSIS ---
def analyze_predictions(preds, targets, class_names=["Down", "Flat", "Up"]):
    """
    Ph√¢n t√≠ch chi ti·∫øt predictions ƒë·ªÉ debug class imbalance.
    """
    preds_np = preds.cpu().numpy()
    targets_np = targets.cpu().numpy()
    
    print("\n" + "="*60)
    print("üìä DETAILED PREDICTION ANALYSIS")
    print("="*60)
    
    # 1. Overall distribution
    pred_counts = np.bincount(preds_np, minlength=3)
    true_counts = np.bincount(targets_np, minlength=3)
    
    print(f"\nüìâ Ground Truth Distribution: {true_counts}")
    print(f"üîÆ Prediction Distribution:   {pred_counts}")
    print(f"   Œî Difference:               {pred_counts - true_counts}")
    
    # 2. Per-class metrics
    print(f"\n{'Class':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
    print("-" * 60)
    
    from sklearn.metrics import precision_recall_fscore_support
    precision, recall, f1, support = precision_recall_fscore_support(
        targets_np, preds_np, labels=[0, 1, 2], zero_division=0
    )
    
    for i, name in enumerate(class_names):
        print(f"{name:<10} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<10}")
    
    # 3. Confusion Matrix
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(targets_np, preds_np, labels=[0, 1, 2])
    
    print("\nüìä Confusion Matrix:")
    print("      Pred Down  Pred Flat  Pred Up")
    for i, name in enumerate(class_names):
        print(f"{name:>8}  {cm[i, 0]:>9}  {cm[i, 1]:>9}  {cm[i, 2]:>7}")
    
    # 4. Critical warnings
    print("\n‚ö†Ô∏è  CRITICAL CHECKS:")
    for i, name in enumerate(class_names):
        if pred_counts[i] == 0:
            print(f"   ‚ùå Class {i} ({name}): NEVER PREDICTED!")
        elif pred_counts[i] < true_counts[i] * 0.3:
            print(f"   ‚ö†Ô∏è  Class {i} ({name}): Severely under-predicted ({pred_counts[i]}/{true_counts[i]})")
        elif recall[i] < 0.1:
            print(f"   ‚ö†Ô∏è  Class {i} ({name}): Recall too low ({recall[i]:.4f})")
    
    print("="*60 + "\n")
    
    return precision, recall, f1

# --- 5. TRAIN ---
def train_model(train_data, valid_data, test_data):
    if not train_data: return

    s_m_dim = train_data["s_m"].shape[-1]
    train_labels = train_data["label"]
    
    # ============================================================
    # üéØ CH·ªåN CHI·∫æN L∆Ø·ª¢C C√ÇN B·∫∞NG
    # ============================================================
    # Uncomment 1 trong 4 options sau:
    
    # OPTION 1: SQRT Balancing (Nh·∫π nh√†ng nh·∫•t - KHUY·∫æN NGH·ªä TH·ª¨ ƒê·∫¶U TI√äN)
    class_weights = compute_class_weights_sqrt(train_labels).to(device)
    strategy_name = "SQRT BALANCING"
    
    # OPTION 2: Effective Number of Samples (C√≥ n·ªÅn t·∫£ng l√Ω thuy·∫øt)
    # class_weights = compute_class_weights_ens(train_labels, beta=0.95).to(device)
    # strategy_name = "EFFECTIVE NUMBER (beta=0.95)"
    
    # OPTION 3: Clipped Inverse Frequency (An to√†n h∆°n inverse thu·∫ßn)
    # class_weights = compute_class_weights_clipped(train_labels, max_ratio=5.0).to(device)
    # strategy_name = "CLIPPED INVERSE (max_ratio=5.0)"
    
    # OPTION 4: T·∫Øt ho√†n to√†n Alpha, ch·ªâ d√πng Focal Loss
    # class_weights = None
    # strategy_name = "FOCAL ONLY (No Alpha)"
    
    # ============================================================
    
    class_counts = np.bincount(train_labels.cpu().numpy())
    print("\n" + "="*60)
    print(f"üéØ BALANCING STRATEGY: {strategy_name}")
    print("="*60)
    print(f"   ‚ñ∫ Class Distribution: {class_counts}")
    if class_weights is not None:
        print(f"   ‚ñ∫ Computed Weights:   {class_weights.cpu().numpy()}")
    else:
        print(f"   ‚ñ∫ Weights: None (Uniform)")
    
    # ============================================================
    # üîß CH·ªåN LOSS FUNCTION
    # ============================================================
    # Uncomment 1 trong 3 options:
    
    # OPTION A: Focal Loss v·ªõi gamma th·∫•p (KHUY·∫æN NGH·ªä)
    use_focal = True
    focal_gamma = 1.0  # Gi·∫£m t·ª´ 2.0 ‚Üí 1.0 ƒë·ªÉ ph·∫°t nh·∫π h∆°n
    use_smoothing = False
    
    # OPTION B: Label Smoothing (Alternative t·ªët)
    # use_focal = False
    # use_smoothing = True
    # focal_gamma = 2.0  # Kh√¥ng d√πng
    
    # OPTION C: Vanilla Cross Entropy
    # use_focal = False
    # use_smoothing = False
    # focal_gamma = 2.0  # Kh√¥ng d√πng
    
    # ============================================================
    
    print(f"\nüöÄ Initializing Model on {device}...")
    
    # ============================================================
    # QUAN TR·ªåNG: N·∫øu model.py ch∆∞a c√≥ focal_gamma, comment d√≤ng ƒë√≥ ƒëi
    # ============================================================
    try:
        # Th·ª≠ kh·ªüi t·∫°o v·ªõi parameters m·ªõi
        model = StockMovementModel(
            price_dim=1,
            macro_dim=s_m_dim,
            news_dim=TrainConfig.news_embed_dim,
            dim=TrainConfig.dim,
            input_dim=TrainConfig.window_size,
            output_dim=TrainConfig.output_dim,
            num_head=TrainConfig.num_head,
            dropout=0.1,
            class_weights=class_weights,
            use_focal_loss=use_focal,
            focal_gamma=focal_gamma,        # [KEY] Gamma ƒëi·ªÅu ch·ªânh ƒë∆∞·ª£c
            use_label_smoothing=use_smoothing,
            smoothing=0.1,
            device=device
        ).to(device)
        print("‚úÖ Using UPDATED model.py with flexible gamma")
    except TypeError:
        # Fallback: D√πng model c≈© (ch·ªâ h·ªó tr·ª£ gamma=2.0 c·ªë ƒë·ªãnh)
        print("‚ö†Ô∏è  Using OLD model.py (gamma fixed at 2.0)")
        print("   üí° To enable flexible gamma, update src/model.py with the artifact code")
        
        model = StockMovementModel(
            price_dim=1,
            macro_dim=s_m_dim,
            news_dim=TrainConfig.news_embed_dim,
            dim=TrainConfig.dim,
            input_dim=TrainConfig.window_size,
            output_dim=TrainConfig.output_dim,
            num_head=TrainConfig.num_head,
            dropout=0.1,
            class_weights=class_weights,
            use_focal_loss=use_focal,
            device=device
        ).to(device)

    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=TrainConfig.learning_rate,
        weight_decay=1e-4
    )

    best_val_mcc = -1.0
    best_val_acc = 0.0
    save_dir = "output"
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "best_model.pt")

    print("\n‚öîÔ∏è  STARTING TRAINING...")
    print("="*60)

    for epoch in range(TrainConfig.epoch_num):
        model.train()
        optimizer.zero_grad()
        
        loss = model(
            train_data["s_o"].to(device), train_data["s_h"].to(device),
            train_data["s_c"].to(device), train_data["s_m"].to(device),
            train_data["s_n"].to(device), train_data["label"].to(device),
            mode="train"
        )
        
        loss.backward()
        if not torch.isfinite(loss): 
            print(f"‚ö†Ô∏è  Training stopped: Loss became {loss.item()}")
            break
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        val_acc, val_mcc = evaluate(model, valid_data)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:03d} | Loss {loss.item():.4f} | Val ACC {val_acc:.4f} | Val MCC {val_mcc:.4f}")

        # L∆∞u model t·ªët nh·∫•t theo MCC (∆∞u ti√™n), ACC (ph·ª•)
        is_best = False
        if val_mcc > best_val_mcc:
            is_best = True
        elif val_mcc == best_val_mcc and val_acc > best_val_acc:
            is_best = True
            
        if is_best:
            best_val_mcc = val_mcc
            best_val_acc = val_acc
            torch.save(model.state_dict(), save_path)
            print(f"   >>> ‚ú® New Best Model! (MCC: {val_mcc:.4f}, Acc: {val_acc:.4f})")

    # =========================================================
    # üèÅ FINAL EVALUATION & DEEP ANALYSIS
    # =========================================================
    print("\n" + "="*60)
    print("üèÅ FINAL EVALUATION")
    print("="*60)
    
    if os.path.exists(save_path):
        model.load_state_dict(torch.load(save_path))
        
        # 1. Validation sanity check
        print("\nüîç VALIDATION SET (Sanity Check):")
        val_acc, val_mcc, val_preds, val_targets = evaluate(model, valid_data, return_details=True)
        print(f"   ACC: {val_acc:.4f} | MCC: {val_mcc:.4f}")
        if val_mcc > 0.1:
            print("   ‚úÖ Model is learning meaningful patterns")
        else:
            print("   ‚ö†Ô∏è  MCC too low - model might be guessing randomly")
        
        # 2. Test set evaluation
        print("\nüîç TEST SET (Final Performance):")
        test_acc, test_mcc, test_preds, test_targets = evaluate(model, test_data, return_details=True)
        print(f"   ACC: {test_acc:.4f} | MCC: {test_mcc:.4f}")
        
        # 3. Deep analysis of test predictions
        if test_preds is not None:
            analyze_predictions(test_preds, test_targets)
        
        # 4. Confidence analysis (optional - ch·ªâ ch·∫°y n·∫øu model h·ªó tr·ª£)
        try:
            print("\nüîç CONFIDENCE ANALYSIS (Sample):")
            model.eval()
            with torch.no_grad():
                probs, preds, confidence = model.get_prediction_confidence(
                    test_data["s_o"][:20].to(device),
                    test_data["s_h"][:20].to(device),
                    test_data["s_c"][:20].to(device),
                    test_data["s_m"][:20].to(device),
                    test_data["s_n"][:20].to(device)
                )
                
                print(f"   Average Confidence: {confidence.mean():.4f}")
                print(f"   Min Confidence: {confidence.min():.4f}")
                print(f"   Max Confidence: {confidence.max():.4f}")
                
                # Check if model is overconfident
                if confidence.mean() > 0.9:
                    print("   ‚ö†Ô∏è  Model might be overconfident!")
                    print("   üí° Consider: Label Smoothing or Temperature Scaling")
        except AttributeError:
            print("\n   (Skipping confidence analysis - update model.py to enable)")
    else:
        print("‚ö†Ô∏è No best model saved.")

if __name__ == "__main__":
    pkl_path = r"D:\ProjectNCKH\deep_finance\data\processed\unified_dataset_test.pkl"
    dp = data_prepare(pkl_path)
    
    target_tickers = ["TSLA", "AMZN", "MSFT", "NFLX"]
    
    list_train, list_valid, list_test = [], [], []
    for ticker in target_tickers:
        try:
            tr, val, te = dp.prepare_data(ticker)
            if tr and len(tr.get("label", [])) > 0:
                list_train.append(tr)
                list_valid.append(val)
                list_test.append(te)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load {ticker}: {e}")

    final_train = merge_datasets(list_train, shuffle=True)
    final_valid = merge_datasets(list_valid, shuffle=False)
    final_test  = merge_datasets(list_test, shuffle=False)

    if len(final_train) > 0:
        train_model(final_train, final_valid, final_test)
    else:
        print("‚ùå No training data available!")