import torch
import random
import numpy as np
import os
from src.model import StockMovementModel
from src.data_loader import data_prepare
from configs.config import TrainConfig

# --- 1. SETUP ---
def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

device = torch.device("cuda" if TrainConfig.use_cuda and torch.cuda.is_available() else "cpu")
set_seed(TrainConfig.seed)

# --- 2. HELPER: MERGE ---
def merge_datasets(list_of_dicts, shuffle=False):
    if not list_of_dicts: return {}
    keys = list_of_dicts[0].keys()
    merged_data = {}
    for key in keys:
        tensors = [d[key] for d in list_of_dicts if d and key in d]
        if tensors:
            merged_data[key] = torch.cat(tensors, dim=0)
    
    if shuffle and "label" in merged_data:
        indices = torch.randperm(len(merged_data["label"]))
        for key in merged_data:
            merged_data[key] = merged_data[key][indices]
    return merged_data

def compute_class_weights(labels_tensor):
    """
    TÃ­nh weights theo phÆ°Æ¡ng phÃ¡p Inverse Class Frequency (Sklearn style).
    Weight_class_i = Total_Samples / (Num_Classes * Count_class_i)
    """
    # Chuyá»ƒn vá» CPU numpy Ä‘á»ƒ tÃ­nh toÃ¡n
    labels = labels_tensor.cpu().numpy()
    class_counts = np.bincount(labels)
    total_samples = len(labels)
    num_classes = len(class_counts)
    
    # TrÃ¡nh chia cho 0 náº¿u lá»¡ cÃ³ class nÃ o rá»—ng (dÃ¹ khÃ³ xáº£y ra vá»›i Z-score)
    class_counts = np.maximum(class_counts, 1) 
    
    weights = total_samples / (num_classes * class_counts)
    
    # Chuyá»ƒn vá» Tensor
    return torch.tensor(weights, dtype=torch.float32)

# --- 3. EVALUATE ---
def evaluate(model, data_dict):
    if not data_dict: return 0.0, 0.0
    model.eval()
    with torch.no_grad():
        s_o = data_dict["s_o"].to(device)
        s_h = data_dict["s_h"].to(device)
        s_c = data_dict["s_c"].to(device)
        s_m = data_dict["s_m"].to(device)
        s_n = data_dict["s_n"].to(device)
        label = data_dict["label"].to(device)
        
        acc, mcc = model(s_o, s_h, s_c, s_m, s_n, label, mode="test")
    return acc, mcc

# --- 4. TRAIN ---
def train_model(train_data, valid_data, test_data):
    if not train_data: return

    s_m_dim = train_data["s_m"].shape[-1]
    
    print("\n  Calculating Class Weights (Balancing Strategy)...")
    train_labels = train_data["label"]
    class_weights = compute_class_weights(train_labels).to(device)
    
    print(f"   â–º Class Counts: {np.bincount(train_labels.cpu().numpy())}")
    print(f"   â–º Computed Alpha: {class_weights.cpu().numpy()}")
    # VÃ­ dá»¥ output: [1.2, 0.6, 1.2] -> Lá»›p Flat (giá»¯a) weight tháº¥p, 2 bÃªn weight cao
    
    print(f"\nðŸš€ Initializing Model on {device}...")
    print(f"   â–º Strategy: FOCAL LOSS (Gamma=2.0) + ALPHA BALANCING")
    
    # KHá»žI Táº O MODEL Vá»šI FOCAL LOSS & KHÃ”NG WEIGHTS
    model = StockMovementModel(
        price_dim=1,
        macro_dim=s_m_dim,
        news_dim=TrainConfig.news_embed_dim,
        dim=TrainConfig.dim,                 # Giáº£m vá» 64 náº¿u cáº§n
        input_dim=TrainConfig.window_size,   
        output_dim=TrainConfig.output_dim,   
        num_head=TrainConfig.num_head,
        dropout=0.1,                         # Dropout vá»«a pháº£i
        class_weights=class_weights,                  # [IMPORTANT] KhÃ´ng dÃ¹ng Weights thá»§ cÃ´ng
        use_focal_loss=True,                   # [IMPORTANT] Báº­t Focal Loss
        device=device
    ).to(device)

    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=TrainConfig.learning_rate, # 1e-3
        weight_decay=1e-4             # Regularization
    )

    best_val_mcc = -1.0 # Theo dÃµi MCC thay vÃ¬ ACC
    save_dir = "output"
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "best_model.pt")

    print("\nâš”ï¸  STARTING TRAINING...")

    for epoch in range(TrainConfig.epoch_num):
        model.train()
        optimizer.zero_grad()
        
        loss = model(
            train_data["s_o"].to(device), train_data["s_h"].to(device),
            train_data["s_c"].to(device), train_data["s_m"].to(device),
            train_data["s_n"].to(device), train_data["label"].to(device),
            mode="train"
        )
        
        loss.backward()
        if not torch.isfinite(loss): break
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        val_acc, val_mcc = evaluate(model, valid_data)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:03d} | Loss {loss.item():.4f} | Val ACC {val_acc:.4f} | Val MCC {val_mcc:.4f}")

        # Æ¯u tiÃªn lÆ°u model cÃ³ MCC cao nháº¥t (trÃ¡nh lÆ°u model Ä‘oÃ¡n bá»«a Mode Collapse)
        is_best = False
        if val_mcc > best_val_mcc:
            is_best = True
        elif val_mcc == best_val_mcc and val_acc > best_val_acc:
            is_best = True
            
        if is_best:
            best_val_mcc = val_mcc
            best_val_acc = val_acc # Cáº­p nháº­t best ACC
            torch.save(model.state_dict(), save_path)
            print(f"   >>> New Best Model Saved! (MCC: {val_mcc:.4f} - Acc: {val_acc:.4f})")

    # =========================================================
    # [UPDATED] FINAL TEST & SANITY CHECK BLOCK
    # =========================================================
    print("\nðŸ FINAL TEST & SANITY CHECK...")
    
    if os.path.exists(save_path):
        # Load láº¡i model tá»‘t nháº¥t
        model.load_state_dict(torch.load(save_path))
        
        # --- BÆ¯á»šC 1: KIá»‚M TRA Láº I TRÃŠN VALID (NÆ¡i ta biáº¿t cháº¯c cháº¯n MCC > 0) ---
        print("ðŸ” Sanity Check on VALID SET:")
        # LÆ°u Ã½: Trong hÃ m nÃ y biáº¿n tÃªn lÃ  'valid_data', khÃ´ng pháº£i 'final_valid'
        val_acc_check, val_mcc_check = evaluate(model, valid_data) 
        print(f"   VALID RESULT -> ACC: {val_acc_check:.4f}, MCC: {val_mcc_check:.4f}")
        

        # --- BÆ¯á»šC 2: CHáº Y TRÃŠN TEST ---
        print("\nðŸ” Run on TEST SET:")
        # LÆ°u Ã½: Trong hÃ m nÃ y biáº¿n tÃªn lÃ  'test_data', khÃ´ng pháº£i 'final_test'
        test_acc, test_mcc = evaluate(model, test_data)
        print(f"ðŸ† TEST RESULT  -> ACC: {test_acc:.4f}, MCC: {test_mcc:.4f}")
        
        # --- BÆ¯á»šC 3: IN RA Dá»° BÃO Cá»¤ THá»‚ (DEBUG) ---
        model.eval()
        with torch.no_grad():
            # Láº¥y 10 máº«u Ä‘áº§u tiÃªn Ä‘á»ƒ xem thá»­ nÃ³ Ä‘oÃ¡n cÃ¡i gÃ¬
            if "s_o" in test_data and len(test_data["s_o"]) > 0:
                print("   (Debug) Checking raw predictions on first batch...")
                # Äoáº¡n nÃ y Ä‘á»ƒ giá»¯ chá»—, náº¿u báº¡n muá»‘n in chi tiáº¿t prediction thÃ¬ cáº§n sá»­a hÃ m evaluate
                # Ä‘á»ƒ tráº£ vá» logits, hoáº·c dÃ¹ng file debug riÃªng.
    else:
        print("âš ï¸ No best model saved.")

if __name__ == "__main__":
    # Cáº­p nháº­t Ä‘Æ°á»ng dáº«n pkl cá»§a báº¡n á»Ÿ Ä‘Ã¢y
    pkl_path = r"D:\DeepFinance\data\processed\unified_dataset_test.pkl" 
    dp = data_prepare(pkl_path)
    
    target_tickers = ["TSLA", "AMZN", "MSFT", "NFLX", "AAPL", "GOOGL", "NVDA", "META"] 
    
    list_train, list_valid, list_test = [], [], []
    for ticker in target_tickers:
        try:
            tr, val, te = dp.prepare_data(ticker)
            if tr and len(tr.get("label", [])) > 0:
                list_train.append(tr); list_valid.append(val); list_test.append(te)
        except: pass

    final_train = merge_datasets(list_train, shuffle=True)
    final_valid = merge_datasets(list_valid, shuffle=False)
    final_test  = merge_datasets(list_test,  shuffle=False)

    if len(final_train) > 0:
        train_model(final_train, final_valid, final_test)