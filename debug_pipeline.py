# =========================================================
# FILE: debug_pipeline.py
# M·ª•c ti√™u: Ph√¢n t√≠ch chi ti·∫øt performance per-ticker
# tr√™n unified_dataset_test.pkl (sau khi build b·∫±ng main_test.py)
# =========================================================

import os
import sys
import numpy as np
import torch
from collections import Counter
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    matthews_corrcoef,
)

# Import project modules
from src.model import StockMovementModel
from src.data_loader import data_prepare
from configs.config import TrainConfig, GlobalConfig

# --- CONFIG ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = os.path.join("output", "best_model.pt")

# ‚úÖ ƒê∆∞·ªùng d·∫´n .pkl l·∫•y t·ª´ GlobalConfig.PROCESSED_PATH
DATA_PATH = os.path.join(
    GlobalConfig.PROCESSED_PATH,
    "unified_dataset_test.pkl",
)


def print_header(title: str):
    print(f"\n{'=' * 60}")
    print(f"üîé {title}")
    print(f"{'=' * 60}")


def load_data_per_ticker(tickers):
    """
    Load d·ªØ li·ªáu Test ri√™ng bi·ªát cho t·ª´ng m√£ ƒë·ªÉ ph√¢n t√≠ch behavior.
    S·ª≠ d·ª•ng logic Rolling Z-Score m·ªõi nh·∫•t t·ª´ data_loader.data_prepare.
    """
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå DATA_PATH not found: {DATA_PATH}")
        print("   ‚Üí H√£y ch·∫°y main_test.py ƒë·ªÉ t·∫°o unified_dataset_test.pkl tr∆∞·ªõc.")
        return {}

    dp = data_prepare(DATA_PATH)
    ticker_datasets = {}

    print(f"üì• Loading TEST data for: {tickers}")
    for t in tickers:
        try:
            # prepare_data tr·∫£ v·ªÅ: train, valid, test
            _, _, test_data = dp.prepare_data(
                stock_name=t,
                window_size=TrainConfig.window_size,
                # C√°c tham s·ªë kh√°c s·∫Ω l·∫•y default t·ª´ Config n·∫øu c√≥
            )

            if test_data and len(test_data.get("label", [])) > 0:
                ticker_datasets[t] = test_data
                print(f"   ‚úÖ {t}: {len(test_data['label'])} samples")
            else:
                print(f"   ‚ö†Ô∏è {t}: No data or empty test set")
        except Exception as e:
            print(f"   ‚ùå {t}: Error {e}")

    return ticker_datasets


def run_prediction(model: StockMovementModel, data_dict: dict):
    """
    Ch·∫°y forward pass ƒë·ªÉ l·∫•y logits + preds + probs.
    ·ªû ƒë√¢y ta t√°i s·ª≠ d·ª•ng ƒë√∫ng pipeline trong model:
      - multimodal_encoder
      - fusion_news / fusion_macro
      - movement_predictor
    """
    model.eval()
    with torch.no_grad():
        s_o = data_dict["s_o"].to(DEVICE)
        s_h = data_dict["s_h"].to(DEVICE)
        s_c = data_dict["s_c"].to(DEVICE)
        s_m = data_dict["s_m"].to(DEVICE)
        s_n = data_dict["s_n"].to(DEVICE)

        # 1. Encoder
        v_m, v_i, v_n = model.multimodal_encoder(s_o, s_h, s_c, s_m, s_n)

        # 2. Fusion (kh·ªõp v·ªõi logic trong StockMovementModel)
        fused_news = model.fusion_news(primary=v_i, aux=v_n)
        fused_macro = model.fusion_macro(primary=v_i, aux=v_m)
        v_fused_total = (fused_news + fused_macro) / 2.0

        # 3. Predictor
        logits = model.movement_predictor(fused_seq=v_fused_total, orig_seq=v_i)

        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)

    return preds.cpu().numpy(), data_dict["label"].numpy(), probs.cpu().numpy()


def analyze_performance():
    # 0. Check data & model t·ªìn t·∫°i
    print_header("0. CHECK FILES")

    if not os.path.exists(DATA_PATH):
        print(f"‚ùå Cannot find dataset at {DATA_PATH}")
        print("   ‚Üí H√£y ch·∫°y main_test.py ƒë·ªÉ t·∫°o unified_dataset_test.pkl tr∆∞·ªõc.")
        return

    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Cannot find model at {MODEL_PATH}")
        print("   ‚Üí H√£y train model b·∫±ng main.py tr∆∞·ªõc.")
        return

    # 1. Load Model
    print_header("1. LOADING MODEL")

    # L·∫•y macro_dim th·ª±c t·∫ø t·ª´ d·ªØ li·ªáu ƒë·ªÉ init model ƒë√∫ng shape
    dp = data_prepare(DATA_PATH)
    dummy_train, _, _ = dp.prepare_data("TSLA")
    if dummy_train and "s_m" in dummy_train:
        macro_dim = dummy_train["s_m"].shape[-1]
    else:
        macro_dim = 6  # fallback n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (√≠t khi d√πng)
    print(f"üîß Model Config: Dim={TrainConfig.dim}, Heads={TrainConfig.num_head}, Macro_dim={macro_dim}")

    model = StockMovementModel(
        price_dim=1,
        macro_dim=macro_dim,
        news_dim=TrainConfig.news_embed_dim,   # ph·∫£i kh·ªõp v·ªõi l√∫c train
        dim=TrainConfig.dim,
        input_dim=TrainConfig.window_size,
        output_dim=TrainConfig.output_dim,
        num_head=TrainConfig.num_head,
        device=DEVICE,
        dropout=0.0,          # eval kh√¥ng c·∫ßn dropout
        class_weights=None,   # eval kh√¥ng t√≠nh loss
        use_focal_loss=False, # eval kh√¥ng d√πng focal
    ).to(DEVICE)

    try:
        state = torch.load(MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(state)
        print("‚úÖ Weights loaded successfully!")
    except Exception as e:
        print(f"‚ùå Error loading weights from {MODEL_PATH}: {e}")
        print("üí° Hint: Ki·ªÉm tra Dim/Heads/news_dim/macro_dim/num_classes trong Config c√≥ kh·ªõp v·ªõi l√∫c train kh√¥ng?")
        return

    # 2. Load Data
    print_header("2. LOADING DATA")
    target_tickers = ["TSLA", "AMZN", "MSFT", "NFLX"]
    datasets = load_data_per_ticker(target_tickers)

    if not datasets:
        print("‚ùå No datasets loaded.")
        return

    # 3. Deep Dive Analysis per ticker
    print_header("3. DEEP DIVE ANALYSIS PER-TICKER")

    all_preds = []
    all_labels = []

    print(
        f"{'TICKER':<10} | {'SAMPLES':<8} | {'ACTUAL (0/1/2)':<20} | "
        f"{'PRED (0/1/2)':<20} | {'ACC':<8} | {'MCC':<8}"
    )
    print("-" * 100)

    for ticker, data in datasets.items():
        preds, labels, probs = run_prediction(model, data)

        all_preds.extend(preds)
        all_labels.extend(labels)

        acc = accuracy_score(labels, preds)
        mcc = matthews_corrcoef(labels, preds)

        act_counts = Counter(labels)
        pred_counts = Counter(preds)

        act_dist = f"{act_counts.get(0,0)}/{act_counts.get(1,0)}/{act_counts.get(2,0)}"
        pred_dist = f"{pred_counts.get(0,0)}/{pred_counts.get(1,0)}/{pred_counts.get(2,0)}"

        print(
            f"{ticker:<10} | {len(labels):<8} | "
            f"{act_dist:<20} | {pred_dist:<20} | {acc:.4f} | {mcc:.4f}"
        )

    # 4. Global Analysis
    print_header("4. GLOBAL SUMMARY (ALL TICKERS COMBINED)")

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    unique_act, counts_act = np.unique(all_labels, return_counts=True)
    unique_pred, counts_pred = np.unique(all_preds, return_counts=True)

    print("üìâ ACTUAL Labels Distribution (Ground Truth):")
    print(f"   {dict(zip(unique_act, counts_act))}")

    print("\nüîÆ PREDICTED Labels Distribution:")
    print(f"   {dict(zip(unique_pred, counts_pred))}")

    # Check mode collapse
    if len(unique_pred) == 1:
        print("\n‚ö†Ô∏è  CRITICAL WARNING: MODE COLLAPSE DETECTED!")
        print(f"   M√¥ h√¨nh ch·ªâ d·ª± ƒëo√°n duy nh·∫•t l·ªõp {unique_pred[0]} cho to√†n b·ªô d·ªØ li·ªáu.")
        print("   ‚Üí ƒê√¢y l√† l√Ω do MCC ~ 0.0 ho·∫∑c r·∫•t th·∫•p.")

    print("\nüìä Confusion Matrix (labels: 0=DOWN, 1=FLAT, 2=UP):")
    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])
    print(f"      Pred 0  Pred 1  Pred 2")
    print(f"Act 0   {cm[0][0]:<7} {cm[0][1]:<7} {cm[0][2]:<7}")
    print(f"Act 1   {cm[1][0]:<7} {cm[1][1]:<7} {cm[1][2]:<7}")
    print(f"Act 2   {cm[2][0]:<7} {cm[2][1]:<7} {cm[2][2]:<7}")

    print("\nüìã Classification Report:")
    print(
        classification_report(
            all_labels,
            all_preds,
            target_names=["DOWN", "FLAT", "UP"],
            zero_division=0,
        )
    )


if __name__ == "__main__":
    analyze_performance()